{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b659a2-4afd-474d-bd56-1a6a72fc1dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 17:35:21.834107: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-19 17:35:21.837907: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-19 17:35:21.880727: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-19 17:35:22.904583: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, FloatType, IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load CIFAR-10 Dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Flatten the labels\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e7fe6e-84c0-480d-949b-58d3274eb6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_using_pyspark(x_train_batch, y_train_batch, x_test_batch, y_test_batch):\n",
    "    \n",
    "    number_train_images = x_train_batch.shape[0]\n",
    "    number_test_images = x_test_batch.shape[0]\n",
    "    \n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"CIFAR10Preprocessing\").getOrCreate()\n",
    "    \n",
    "    #print(x_train_batch.shape, x_test_batch.shape)\n",
    "    \n",
    "    # Prepare data as a list of tuples\n",
    "    train_data = [(x_train_batch[j].tolist(), int(y_train_batch[j])) for j in range(len(y_train_batch))]\n",
    "    test_data = [(x_test_batch[j].tolist(), int(y_test_batch[j])) for j in range(len(y_test_batch))]\n",
    "    \n",
    "    # Define the schema\n",
    "    schema = StructType([\n",
    "        StructField('image', ArrayType(ArrayType(ArrayType(IntegerType()))), False),\n",
    "        StructField('label', IntegerType(), False)\n",
    "    ])\n",
    "    \n",
    "    # Create Spark DataFrames\n",
    "    train_df = spark.createDataFrame(train_data, schema)\n",
    "    test_df = spark.createDataFrame(test_data, schema)\n",
    "    \n",
    "    # Define the normalization UDF\n",
    "    def normalize_images(image):\n",
    "        image = np.array(image).astype('float32') / 255.0  # Normalization step\n",
    "        return image.tolist()\n",
    "    \n",
    "    normalize_udf = udf(normalize_images, ArrayType(ArrayType(ArrayType(FloatType()))))\n",
    "    \n",
    "    # Apply the normalization UDF\n",
    "    train_df = train_df.withColumn('image', normalize_udf(train_df['image']))\n",
    "    test_df = test_df.withColumn('image', normalize_udf(test_df['image']))\n",
    "    \n",
    "    train_numpy =np.array(train_df.select('image').collect())\n",
    "    test_numpy =np.array(test_df.select('image').collect())\n",
    "\n",
    "    train_numpy = train_numpy.reshape(number_train_images,32,32,3)\n",
    "    test_numpy = test_numpy.reshape(number_test_images,32,32,3)\n",
    "    \n",
    "    #print(train_df_numpy.shape)\n",
    "    \n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "\n",
    "    return [train_numpy,test_numpy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72067ef9-6c8f-40da-b1bb-3a345b55aac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 17:35:39 WARN Utils: Your hostname, dell-Inspiron-5584 resolves to a loopback address: 127.0.1.1; using 192.168.38.254 instead (on interface wlp2s0)\n",
      "24/05/19 17:35:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/19 17:35:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/19 17:36:05 WARN TaskSetManager: Stage 0 contains a task of very large size (10141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/19 17:36:43 WARN TaskSetManager: Stage 1 contains a task of very large size (1274 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape of train data: (8000, 32, 32, 3)\n",
      "Current shape of test data: (1000, 32, 32, 3)\n",
      "Batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 17:37:10 WARN TaskSetManager: Stage 0 contains a task of very large size (10141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/19 17:37:49 WARN TaskSetManager: Stage 1 contains a task of very large size (1274 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape of train data: (16000, 32, 32, 3)\n",
      "Current shape of test data: (2000, 32, 32, 3)\n",
      "Batch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 17:38:14 WARN TaskSetManager: Stage 0 contains a task of very large size (10141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/19 17:38:53 WARN TaskSetManager: Stage 1 contains a task of very large size (1274 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape of train data: (24000, 32, 32, 3)\n",
      "Current shape of test data: (3000, 32, 32, 3)\n",
      "Batch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 17:39:19 WARN TaskSetManager: Stage 0 contains a task of very large size (10141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/19 17:39:57 WARN TaskSetManager: Stage 1 contains a task of very large size (1274 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape of train data: (32000, 32, 32, 3)\n",
      "Current shape of test data: (4000, 32, 32, 3)\n",
      "Batch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 17:40:24 WARN TaskSetManager: Stage 0 contains a task of very large size (10141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/19 17:41:02 WARN TaskSetManager: Stage 1 contains a task of very large size (1274 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape of train data: (40000, 32, 32, 3)\n",
      "Current shape of test data: (5000, 32, 32, 3)\n",
      "Batch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 17:41:31 WARN TaskSetManager: Stage 0 contains a task of very large size (10141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/19 17:42:08 WARN TaskSetManager: Stage 1 contains a task of very large size (1274 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape of train data: (48000, 32, 32, 3)\n",
      "Current shape of test data: (6000, 32, 32, 3)\n",
      "Batch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 17:42:29 WARN TaskSetManager: Stage 0 contains a task of very large size (2540 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/19 17:42:38 WARN TaskSetManager: Stage 1 contains a task of very large size (5074 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape of train data: (50000, 32, 32, 3)\n",
      "Current shape of test data: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Batch-wise data processing\n",
    "for i in range(7):\n",
    "    print('Batch',i)\n",
    "    if i<6:\n",
    "        train_batch_numpy,test_batch_numpy = preprocessing_using_pyspark(x_train[i*8000:(i+1)*8000], y_train[i*8000:(i+1)*8000], x_test[i*1000:(i+1)*1000],y_test[i*1000:(i+1)*1000])\n",
    "    else:\n",
    "        train_batch_numpy,test_batch_numpy = preprocessing_using_pyspark(x_train[i*8000:50000], y_train[i*8000:50000], x_test[i*1000:10000],y_test[i*1000:10000])\n",
    "        \n",
    "    if i==0:\n",
    "        x_train_normalized = train_batch_numpy\n",
    "        x_test_normalized = test_batch_numpy\n",
    "    else:\n",
    "        x_train_normalized = np.concatenate((x_train_normalized, train_batch_numpy), axis=0)\n",
    "        x_test_normalized = np.concatenate((x_test_normalized, test_batch_numpy), axis=0)\n",
    "\n",
    "    print('Current shape of train data:',x_train_normalized.shape)\n",
    "    print('Current shape of test data:',x_test_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7169483d-0bef-4234-ae0c-60097125a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving processed data in npz file\n",
    "np.savez('cifar_dataset_processed.npz',x_train = x_train_normalized, y_train = y_train, x_test = x_test_normalized, y_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba5923-c28f-4282-83f6-184a4926f93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
